# LLM

## 模型相關論文

- [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)
  - InstructGPT
  - OpenAI
- [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068.pdf)
  - OPT
  - Meta
- [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf)
  - Flan-T5/PaLM
  - Google
- [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/pdf/2211.05100.pdf)
  - BLOOM
  - BigScience
- [Galactica: A Large Language Model for Science](https://arxiv.org/pdf/2211.09085.pdf)
  - Galactica
  - Meta
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)
  - LLaMA
  - Meta
- [Baize: An Open-Source Chat Model with Parameter-Efficient Tuning on Self-Chat Data](https://arxiv.org/pdf/2304.01196v2.pdf)

## 演算法相關論文

- [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)
- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf)
- [MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning](https://arxiv.org/pdf/2205.00445.pdf)
- [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/pdf/2210.03629.pdf)
- [PAL: Program-aided Language Models](https://arxiv.org/pdf/2211.10435.pdf)
- [Large Language Models are Versatile Decomposers: Decompose Evidence and Questions for Table-based Reasoning](https://arxiv.org/pdf/2301.13808.pdf)
- [Multimodal Chain-of-Thought Reasoning in Language Models](https://arxiv.org/pdf/2302.00923.pdf)
- [Toolformer: Language Models Can Teach Themselves to Use Tools](https://arxiv.org/pdf/2302.04761.pdf)
- [SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks](https://arxiv.org/pdf/2302.13939.pdf)
- [Goal Driven Discovery of Distributional Differences via Language Descriptions](https://arxiv.org/pdf/2302.14233.pdf)
- [Reward Design with Language Models](https://arxiv.org/pdf/2303.00001.pdf)
- [MathPrompter: Mathematical Reasoning using Large Language Models](https://arxiv.org/pdf/2303.05398.pdf)
- [Self-planning Code Generation with Large Language Model](https://arxiv.org/pdf/2303.06689.pdf)
- [Sparks of Artificial General Intelligence: Early experiments with GPT-4](https://arxiv.org/pdf/2303.12712.pdf)
- [HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face](https://arxiv.org/pdf/2303.17580.pdf)
- [CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society](https://arxiv.org/pdf/2303.17760.pdf)
- [Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/pdf/2304.03442.pdf)
- [Low-code LLM: Visual Programming over LLMs](https://arxiv.org/pdf/2304.08103.pdf)
- [Supporting Human-AI Collaboration in Auditing LLMs with LLMs](https://arxiv.org/pdf/2304.09991.pdf)
- [Learning to Program with Natural Language](https://arxiv.org/pdf/2304.10464.pdf)
- [Scaling Transformer to 1M tokens and beyond with RMT](https://arxiv.org/pdf/2304.11062.pdf)
- [LLM+P: Empowering Large Language Models with Optimal Planning Proficiency](https://arxiv.org/pdf/2304.11477.pdf)
- [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/pdf/2304.12244.pdf)
- [Multi-Party Chat: Conversational Agents in Group Settings with Humans and Models](https://arxiv.org/pdf/2304.13835.pdf)
- [Extracting Structured Seed-Mediated Gold Nanorod Growth Procedures from Literature with GPT-3](https://arxiv.org/pdf/2304.13846.pdf)
- [q2d: Turning Questions into Dialogs to Teach Models How to Search](https://arxiv.org/pdf/2304.14318.pdf)
- [We’re Afraid Language Models Aren’t Modeling Ambiguity](https://arxiv.org/pdf/2304.14399.pdf)
- [Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks](https://arxiv.org/pdf/2304.14732.pdf)
- [Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation](https://arxiv.org/pdf/2305.01210.pdf)
- [AutoML-GPT: Automatic Machine Learning with GPT](https://arxiv.org/pdf/2305.02499.pdf)
- [ZipIt! Merging Models from Different Tasks without Training](https://arxiv.org/pdf/2305.03053.pdf)
- [Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs](https://arxiv.org/pdf/2305.03111.pdf)
- [Automatic Prompt Optimization with "Gradient Descent" and Beam Search](https://arxiv.org/pdf/2305.03495.pdf)
- [Can Large Language Models Transform Computational Social Science?](https://arxiv.org/pdf/2305.03514.pdf)
- [A Suite of Generative Tasks for Multi-Level Multimodal Webpage Understanding](https://arxiv.org/pdf/2305.03668.pdf)
- [Otter: A Multi-Modal Model with In-Context Instruction Tuning](https://arxiv.org/pdf/2305.03726.pdf)
- [Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control](https://grounded-decoding.github.io/)

## Repositories

- [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM)
- [JARVIS](https://github.com/microsoft/JARVIS)
- [visual-chatgpt](https://github.com/microsoft/visual-chatgpt)
- [Auto-GPT](https://github.com/Torantulino/Auto-GPT)
- [babyagi](https://github.com/yoheinakajima/babyagi)
- [vocode](https://github.com/vocodedev/vocode-python)
- [langflow](https://github.com/logspace-ai/langflow)
- [langchain-prefect](https://github.com/PrefectHQ/langchain-prefect)
- [langchain-visualizer](https://github.com/amosjyng/langchain-visualizer)
- [langchain-ui](https://github.com/haneyume/langchain-ui)
- [langchain-serve](https://github.com/haneyume/langchain-serve)
- [flux](https://github.com/transmissions11/flux)
- [Prompt Engineering Guide](https://github.com/dair-ai/Prompt-Engineering-Guide)
- [FlexGen](https://github.com/FMInference/FlexGen)
- [gpt4all](https://github.com/nomic-ai/gpt4all)
- [RWKV-LM](https://github.com/BlinkDL/RWKV-LM)
- [xturing](https://github.com/stochasticai/xturing)
- [chat-flow](https://github.com/prompt-engineering/chat-flow)
- [tabby](https://github.com/TabbyML/tabby)
- [openplayground](https://github.com/nat/openplayground)
- [OpenICL](https://github.com/Shark-NLP/OpenICL)
- [chatarena](https://github.com/chatarena/chatarena)
- [segment-anything-video](https://github.com/kadirnar/segment-anything-video)
- [BabyDORA](https://github.com/ttizze/BabyDORA)
- [Grounded-Segment-Anything](https://github.com/IDEA-Research/Grounded-Segment-Anything)
- [AgentGPT](https://github.com/reworkd/AgentGPT)
- [yakGPT](https://github.com/yakGPT/yakGPT)
- [aicommits](https://github.com/Nutlope/aicommits)
- [EditAnything](https://github.com/sail-sg/EditAnything)
- [crustagi](https://github.com/lukaesch/crustagi)
- [ai-legion](https://github.com/eumemic/ai-legion)
- [GPTCache](https://github.com/zilliztech/GPTCache)
- [wolverine](https://github.com/biobootloader/wolverine)
- [dev-gpt](https://github.com/jina-ai/dev-gpt)
- [auto-gpt-web](https://github.com/jina-ai/auto-gpt-web)
- [Streamlit-Image-Annotation](https://github.com/hirune924/Streamlit-Image-Annotation)
- [kor](https://github.com/eyurtsev/kor)
- [camel](https://github.com/lightaime/camel)
- [pyCodeAGI](https://github.com/chakkaradeep/pyCodeAGI)
- [LlamaAcademy](https://github.com/danielgross/LlamaAcademy)
- [NeMo-Guardrails](https://github.com/NVIDIA/NeMo-Guardrails)
- [xai-gpt-agent-toolkit](https://github.com/XpressAI/xai-gpt-agent-toolkit)
- [AI-TDD](https://github.com/di-sukharev/AI-TDD)
- [ai-component-generator](https://github.com/haneyume/ai-component-generator)
- [thinkgpt](https://github.com/alaeddine-13/thinkgpt)
- [ChatVRM](https://github.com/pixiv/ChatVRM)
- [MiniChain](https://github.com/srush/MiniChain)
- [StoryStorm](https://github.com/peterw/StoryStorm)
- [langcorn](https://github.com/msoedov/langcorn)
- [TaskMatrix](https://github.com/microsoft/TaskMatrix)
- [FigmaChain](https://github.com/cirediatpl/FigmaChain)
- [SadTalker](https://github.com/OpenTalker/SadTalker)
- [rxdrag](https://github.com/codebdy/rxdrag)

## 文章

- [Learn Prompting](https://learnprompting.org/docs/intro)
- [何謂 Transformer 模型？](https://blogs.nvidia.com.tw/2022/06/21/what-is-a-transformer-model/)
- [The Transformer Architecture](https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html)
- [d2l](https://d2l.ai/)
- [Llama Hub](https://llamahub.ai/)

## Websites

- [OpenAI](https://openai.com/)
- [Hugging Face](https://huggingface.co/)
- [Papers With Code](https://paperswithcode.com/)
- [LangChain](https://python.langchain.com/en/latest/)
- [LangChainJs](https://js.langchain.com/docs/)
- [Vocode](https://docs.vocode.dev/welcome)
- [Runway](https://runwayml.com/)
- [Jina](https://jina.ai/)
- [Cohere](https://cohere.ai/)
- [Deepgram](https://deepgram.com/)
- [Futurepedia](https://www.futurepedia.io/)
- [Prompt Engineering Guide](https://www.promptingguide.ai/zh)
- [ExplainThis](https://www.explainthis.io/zh-hant/chatgpt)
- [Chat with Open Large Language Models](https://chat.lmsys.org/)
- [Task-driven Autonomous Agent Utilizing GPT-4, Pinecone, and LangChain for Diverse Applications](https://yoheinakajima.com/task-driven-autonomous-agent-utilizing-gpt-4-pinecone-and-langchain-for-diverse-applications/)
- [Godmode.space](https://godmode.space/)
- [Store Copilot](https://copilot.blanklob.com/)
- [Human-AI Interaction](https://creativity.ucsd.edu/ai)

## Database for Vector Search

- [Chroma](https://docs.trychroma.com/)
- [Pinecone](https://www.pinecone.io/)
- [Weaviate](https://weaviate.io/)

## Code Examples

- [Text classification with Transformer](https://keras.io/examples/nlp/text_classification_with_transformer/)

## 知乎

- [五万字综述！Prompt-Tuning：深度解读一种新的微调范式](https://zhuanlan.zhihu.com/p/618871247)
- [实时软件生成 —— Prompt 编程能否打通低代码的最后一公里？](https://zhuanlan.zhihu.com/p/610865447)
- [无代码编程](https://zhuanlan.zhihu.com/p/61288928)
- [如何从浅入深理解 transformer？](https://www.zhihu.com/question/471328838/answer/2864224369)
- [ChatGPT：从描述世界到创造世界](https://zhuanlan.zhihu.com/p/619291742)
- [Meta 发布图像分割论文 Segment Anything，将给 CV 研究带来什么影响？](https://www.zhihu.com/question/593914819/answer/2971080467)
- [AI 研发提效的正确姿势：开源 LLM + LoRA](https://zhuanlan.zhihu.com/p/620236884)
